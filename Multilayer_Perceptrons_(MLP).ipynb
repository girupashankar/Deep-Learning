{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjF5L+e6issz429WcJLpvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girupashankar/Deep-Learning/blob/main/Multilayer_Perceptrons_(MLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptrons (MLP)\n",
        "\n",
        "#### Overview\n",
        "A Multilayer Perceptron (MLP) is a class of feedforward artificial neural networks (FNN). An MLP consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Each node (except for the input nodes) is a neuron that uses a nonlinear activation function.\n",
        "\n",
        "#### Key Components\n",
        "1. **Input Layer**: The layer that receives the input data.\n",
        "2. **Hidden Layers**: One or more intermediate layers that process inputs through weighted connections. These layers enable the network to learn complex patterns.\n",
        "3. **Output Layer**: The final layer that produces the prediction or classification.\n",
        "4. **Activation Functions**: Functions applied to the output of each neuron to introduce non-linearity (e.g., ReLU, Sigmoid, Tanh).\n",
        "\n",
        "#### Training Process\n",
        "1. **Forward Propagation**: Compute the output of the network for a given input by passing the input through each layer.\n",
        "2. **Loss Calculation**: Measure the error between the predicted output and the actual target.\n",
        "3. **Backward Propagation**: Adjust the weights and biases to minimize the error by using gradient descent and backpropagation algorithms.\n",
        "\n",
        "### Real-World Examples\n",
        "\n",
        "1. **Image Classification**\n",
        "   - **Problem**: Classify images into categories (e.g., recognizing handwritten digits).\n",
        "   - **Application**: Digit recognition for postal services or OCR systems.\n",
        "\n",
        "2. **Medical Diagnosis**\n",
        "   - **Problem**: Predict the presence of diseases from patient data (e.g., diabetes prediction).\n",
        "   - **Application**: Assisting doctors in diagnosing medical conditions based on patient records.\n",
        "\n",
        "3. **Stock Market Prediction**\n",
        "   - **Problem**: Predict stock prices based on historical data and indicators.\n",
        "   - **Application**: Financial analysis and automated trading systems.\n",
        "\n",
        "### Code Example: Handwritten Digit Recognition\n",
        "\n",
        "Here is an example using Python with TensorFlow and Keras to create an MLP for recognizing handwritten digits from the MNIST dataset.\n",
        "\n",
        "\n",
        "### Explanation\n",
        "1. **Data Loading and Preprocessing**: The MNIST dataset is loaded and normalized. The labels are one-hot encoded to make them suitable for classification.\n",
        "2. **Model Building**: An MLP is built with an input layer (Flatten layer), two hidden layers with ReLU activation, and an output layer with softmax activation.\n",
        "3. **Compilation and Training**: The model is compiled with the Adam optimizer and categorical cross-entropy loss. It is then trained on the training data.\n",
        "4. **Evaluation**: The model's performance is evaluated on the test set, and the test accuracy is printed.\n",
        "5. **Prediction**: Predictions are made on a few test samples, and the results are compared to the actual labels.\n",
        "6. **Visualization**: The training and validation accuracy over epochs are plotted to visualize the learning process.\n",
        "\n",
        "This example demonstrates how to implement a basic MLP for a classification task, specifically recognizing handwritten digits. The same approach can be adapted for other types of classification and regression problems by modifying the network architecture and preprocessing steps."
      ],
      "metadata": {
        "id": "Y-lVirnf21J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into a 784-element vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons\n",
        "    Dense(64, activation='relu'),   # Second hidden layer with 64 neurons\n",
        "    Dense(10, activation='softmax') # Output layer with 10 neurons for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Predicting on a few test samples\n",
        "predictions = model.predict(X_test[:5])\n",
        "\n",
        "# Displaying the predictions alongside actual labels\n",
        "for i in range(5):\n",
        "    print(f'Actual Label: {np.argmax(y_test[i])}, Predicted Label: {np.argmax(predictions[i])}')\n",
        "\n",
        "# Visualize training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnPTfYjh2_1B",
        "outputId": "bd45cea8-ee61-4443-dcd6-ee580776c75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.2663 - accuracy: 0.9231 - val_loss: 0.1471 - val_accuracy: 0.9553\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1101 - accuracy: 0.9674 - val_loss: 0.1199 - val_accuracy: 0.9617\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0747 - accuracy: 0.9780 - val_loss: 0.0903 - val_accuracy: 0.9722\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0561 - accuracy: 0.9821 - val_loss: 0.1034 - val_accuracy: 0.9707\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0437 - accuracy: 0.9857 - val_loss: 0.0944 - val_accuracy: 0.9743\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0344 - accuracy: 0.9887 - val_loss: 0.1122 - val_accuracy: 0.9703\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0274 - accuracy: 0.9912 - val_loss: 0.1061 - val_accuracy: 0.9730\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 0.1123 - val_accuracy: 0.9729\n",
            "Epoch 9/10\n",
            " 536/1500 [=========>....................] - ETA: 3s - loss: 0.0174 - accuracy: 0.9945"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwqkm__j3Asz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}